
<!DOCTYPE html>
<html>
    <head>
        
        <title>Collisteru</title>
        <meta charset="UTF-8">
        <meta name="description" content=" Diff. Eqs ">
        <meta name="author" content="Sean Carter">
        <link rel="stylesheet" type="text/css" href="../css/style.css" title="style"> 
    </head>
    <body>
        <div id="headbox">
                <a href="https://collisteru.github.io/index.html" class="headerLink" > <div><b>Collisteru</b></div></a> 
                <a href="https://collisteru.github.io/writing/writingDirectory.html" class="headerLink" > <div>Writing</div></a> 
                <a href="https://collisteru.github.io/contact.html" class="headerLink" > <div>Contact</div></a> 
            </div>
        <div class="upperScroll"> </div>
        <div id="mainbox">

        <p><em>Disclaimer:</em></p>
<p>These notes were written mostly for my own use and might not make sense to everyone. They miss certain topics (like modeling) and go perhaps too deep into others depending on my interests and research. However, it is often helpful to learn a concept in more than one way. If my voice is the one that makes it click for you, then this is well worth it.</p>
<h1 id="notes-on-math-4b-differential-equations-part-i">Notes on MATH 4B – Differential Equations – Part I</h1>
<p>Differential equations describe relationships between functions and their derivatives. We solve them by finding the set of functions that make the relationship true. Differential equations are important to us, as engineers and applied mathematicians, because they appear over and over again in science but are difficult to solve. In fact, <a href="https://math.stackexchange.com/questions/https://math.stackexchange.com/questions/3782499/is-there-a-reason-it-is-so-rare-we-can-solve-differential-equations3782499/is-there-a-reason-it-is-so-rare-we-can-solve-differential-equations">the vast majority of differential equations are not solvable analytically</a>. This class covers the tricks we use for the ones that are.</p>
<p>Notational note: for this class, t is the independent variable and we solve for y(t)</p>
<h2 id="classifying-differential-equations">Classifying Differential Equations</h2>
<p>Classifying differential equations helps us quickly recognize patterns in the solvable ones.</p>
<p>We classify differential equations on three axes: <strong>Order</strong>, <strong>Linear vs. Nonlinear</strong>, and <strong>Ordinary vs. Partial</strong></p>
<p>The <strong>order</strong> of a differential equation is the largest derivative present in the equation. A differential equation is <strong>linear</strong> iff it can be expressed in the following form:</p>
$$
n{bmatrix}
a_1 & a_2 & a_3 & a_4 & ... & a_{(n)}
\end{bmatrix}
\cdot 
egin{bmatrix}
1\ 
y\ 
y^{\prime}\
        y^{\prime\prime}\
        ...\
        y^{(n)}\ 

        \end{bmatrix}
        =g(t)

    \]
$$
<p>In other words, the function y and its derivatives are (1) never raised to any power other than the first and (2) don’t appear “inside” other functions like \(y\)</p>
<p>A differential equation is partial if it contains partial derivatives; it is ordinary if it contains ordinary (univariate) derivatives. This class focuses on ordinary differential equations, or ODEs.</p>
<p>Note that there are also systems of differential equations in which we consider more than one at a time. They can typically be rewritten as differential equations involving vectors.</p>
<h2 id="examples-of-differential-equations">Examples of Differential Equations</h2>
<ul>
<li>Newton’s Second Law, expressed as</li>
</ul>
$$
F(x, y) = m \cdot x''
$$
<ul>
<li>Verhulst’s famous equation for logistic growth of individual species</li>
<li>Fourier’s Heat Equation</li>
</ul>
<h2 id="solving-differential-equations">Solving Differential Equations</h2>
<p>One of the most interesting features of differential equations is that they usually have <em>an infinite number of solutions</em>. Each one corresponds to a different value for the constant of integration, which you’ll remember from Calculus I and which we introduce when solving ODEs.</p>
<p>When your teacher asks for the <strong>general solution</strong> of an ODE, they want the formula (or set thereof) that describes all the solutions. These formulas leave the constant of integration unspecified. When you assign it a value, you transform the general solution into a <strong>real solution</strong>.</p>
<p>Watch out, however: discontinuities can introduce the need for more than one equation in the general solution. You need them all to be correct.</p>
<p>A linear algebraist would say that solutions to ODEs form a subspace of the set of continuously differential functions. For general solutions with only one constant, that subspace has only one dimension.</p>
<p><strong>Direction Fields</strong> illustrate this principle beautifully. They chart how <br /><span class="math display"><em>y</em><sup>′</sup></span><br /> varies along the <br /><span class="math display"><em>t</em></span><br /> and <br /><span class="math display"><em>y</em></span><br /> axes. The ODE’s solutions follow the slopes like gulls over gulf steams.</p>
<p>It is also no accident that direction fields resemble vector fields. Differential equations illustrate a specific path in the general fluid flow that vector fields express. This is one of the features that makes them indispensible to science.</p>
<h2 id="solving-linear-odes">Solving Linear ODEs</h2>
<h3 id="homogeneous-linear-odes">Homogeneous Linear ODEs</h3>
<p>The right-hand side is zero and the left-hand side is a linear combination of the function and its derivatives. One example looks like the following:</p>
$$
x' = \sin(2x)
$$
<p>Just integrate both sides. You can do this if you have an arbitrary-order derivative of <span class="math inline"><em>y</em></span> on one side and a <span class="math inline"><em>g</em>(<em>t</em>)</span> on the other.</p>
<h3 id="nonhomogeneous-linear-odes">Nonhomogeneous Linear ODEs</h3>
<p>You can also solve a nonhomogeneous first-order linear ODE in the following form:</p>
$$y^{\prime} + p(t) y = g(t)$$
<p>… by multiplying all terms by an integrating factor.</p>
<p>This integrating factor is <br /><span class="math display">$$\mu = e^(\integral{f(t)} $$</span><br />.</p>
<p>And the solution is:</p>
$$y = rac{1}{\mu} (\int \mu g + C) $$
<p>(Usually the hardest part of this type of problem is integrating <span class="math inline"><em>g</em>(<em>t</em>)</span>, which may require integration by parts, a complicated U-substitution, or both.</p>
<p>Once you’ve solved for an ODE’s general solution, an initial value problem will give you an expression of the form <span class="math inline"><em>y</em>(<em>a</em>) = <em>b</em></span> and ask you to solve for C as well.</p>
<ul>
<li>WATCH OUT: Solutions containing t raised to negative or fractional powers are often domain-limited. Check for singularities or individual constant solutions like \(t = 0\).</li>
</ul>
<h2 id="separation-of-variables">Separation of Variables</h2>
<p>You can do this if your equation can be expressed as:</p>
$$ F(t, y) = T(t)Y(t)$$
<p>In this case, you can manipulate it into this form:</p>
$$rac{1}{Y(y)} dy = T(t)dt $$
<p>My warning from earlier applies here with special force. The manipulation above often results in domain-limited curves. After you solve for the general solution, always check the domain. You may need to solve for y’s domain, then backsolve for the restrictions on x.</p>
<h2 id="exact-equations">Exact Equations</h2>
<p>An <em>exact equation</em> appers in standard form like this:</p>
$$
M(t, y) dt + N(t, y) dy = 0
$$
<p>Where there exists a function <br /><span class="math display"><em>ψ</em></span><br /> such that such that:</p>
$$
M = rac{\partial \psi}{\partial t} , N = rac{\partial \psi}{\partial y}
$$
<p>We call <br /><span class="math display"><em>ψ</em></span><br /> the <strong>scalar potential</strong>, and a theorem by Poincaré guarantees its existence so long as <br /><span class="math display"><em>M</em><sub><em>y</em></sub> = <em>N</em><sub><em>t</em></sub></span><br />.</p>
<p>If you know that <br /><span class="math display"><em>M</em><sub><em>y</em></sub> = <em>N</em><sub><em>t</em></sub></span><br />, one way to find the scalar potential is just to look at M and N and find by inspection a function <br /><span class="math display"><em>ψ</em></span><br /> such that <br /><span class="math display"><em>ψ</em><sub><em>x</em></sub> = <em>M</em></span><br /> and <br /><span class="math display"><em>ψ</em><sub><em>y</em></sub> = <em>N</em></span><br /></p>
<p>Another method is a line integral, where the line is an L-shape that goes only horizontally at first from (0, 0) to (x, 0) and then all vertically from (x, 0) to (x, y).</p>
<h3 id="making-exact-equations-with-integrating-factors">Making Exact Equations with Integrating Factors</h3>
<p>We can sometimes turn a non-exact equation into one that is exact with an integrating factor. We locate a positive function <br /><span class="math display"><em>μ</em>(<em>t</em>, <em>y</em>)</span><br /> such that</p>
$$(\mu M )_y = ( \mu N)_t$$
<p>This equation is <em>usually</em> harder to solve than the original ODE. There are two cases when it isn’t: when <span class="math inline"><em>μ</em></span> is a function solely of <span class="math inline"><em>t</em></span> and when it is a function solely of <span class="math inline"><em>y</em></span>. 
<h2 id="change-of-variables">Change of Variables</h2>
<p>In some cases, a change of variables can convert one type of ODE into another type that we can solve.</p>
<h3 id="rationically-homogeneous-equations-rhes">Rationically Homogeneous Equations (RHEs)</h3>
<p>I call them “rationically homogeneous” even though they’re actually just called “homogeneous” to distinguish them from the homogeneous equations discussed earlier.</p>
<p>RHEs come in the form <br /><span class="math display"><em>y</em><sup>′</sup> = <em>F</em>(<em>t</em>, <em>y</em>)</span><br />, where F depends only on the ratio <span class="math inline"><em>y</em>/<em>t</em></span>.</p>
<p>We solve this with the change of variables <br /><span class="math display">$$v = rac{y}{t} $$</span><br />.</p>
<h3 id="bernoulli-differential-equations">Bernoulli Differential Equations</h3>
<p>This is our first encounter with non-first order differential equations. <strong>Bernoulli Equations</strong> come in the form:</p>
<p><br /><span class="math display"><em>y</em><sup>′</sup> + <em>p</em>(<em>x</em>)<em>y</em> = <em>q</em>(<em>x</em>)<em>y</em><sup><em>n</em></sup></span><br /></p>
<p>The crucial step to solving these is dividing the whole equation by <br /><span class="math display"><em>y</em><sup><em>n</em></sup></span><br />, leaving</p>
<p><br /><span class="math display"><em>y</em><sup> − <em>n</em></sup><em>y</em><sup>′</sup> + <em>p</em>(<em>x</em>)<em>y</em><sup>1 − <em>n</em></sup> = <em>q</em>(<em>x</em>)</span><br /></p>
<p>This isolates the RHS, allowing us to transform the equation into a classic linear first-order ODE with the following substitution:</p>
<p><br /><span class="math display"><em>v</em> = <em>y</em><sup>1 − <em>n</em></sup></span><br /></p>
<p>which turns the above into</p>
<p><br /><span class="math display">$$rac{v^{\prime}}{(1-n)} + vp(x) = q(x) $$</span><br /></p>
<p>and solve.</p>
<h3 id="riccati-equations">Riccati Equations</h3>
<p>The equation</p>
<p><br /><span class="math display"><em>y</em>′ = <em>a</em><sub>0</sub>(5) + <em>a</em><sub>1</sub>(<em>t</em>)<em>y</em> + <em>a</em><sub>2</sub>(<em>t</em>)<em>y</em><sup>2</sup></span><br /></p>
<p>is a <strong>Riccati equation</strong> if \(a_0\), \(a_1\), \(a_2\) are continuous and \(a_2\) is not zero (if is, the equation is just linear, and we can solve it using methods we already know.)</p>
<p>We can solve Riccati equations by knowing a <em>particular</em> solution \(y_p\). Rewrite the equation using \(t\) and \(v\), where</p>
<p><br /><span class="math display"><em>v</em> = <em>y</em> − <em>y</em><sub><em>p</em></sub></span><br /></p>
<p>Then, solve this for \(y\) and replace \(v\) back to return to the original solution.</p>
<h3>Autonomous Differential Equations</h3>
<p>Some differential equations don’t depend directly on t but instead only on y.These are known as <strong>autonomous</strong> and often crop up in nature.</p>
<p>Consider a poulation of jackrabbits in a field of boundless plenty. The only restriction on their growth is the number of rabbits available to breed and have children. The more rabbits there are, the more rabbit couplings can occur per gestational period and the more quickly the population grows. We model this with the equation below:</p>
<p><br /><span class="math display">$$
rac{dy}{dx} = ay
$$</span><br /></p>
<p>These can usually be solved with separation of variables.</p>
<h1 id="by-power-series">By Power Series</h1>
<p>Sometimes we can convert an equation into a power series. Differentiate the equation to get another power series, and you can sometimes combine that information to get an expression that reproduces the coefficients. Even luckier, you might be able to use one of the series convergence formulas to get a continuous solution.</p>
<h1 id="the-fundamental-theorem-of-ordinary-differential-equations">The Fundamental Theorem of Ordinary Differential Equations</h1>
<p>All of the equations we’ve looked at so far have been “nice,” in that they’ve been carefully designed to fall into one of the forms that we can solve relatively easily. However, the general differential equation will not be so nice– in fact, there are many that we don’t know how to solve with analytic (algebraic) methods.</p>
<p>For all this difficulty, however, there is a ray of hope. <em>The Fundamental Theorem of Ordinary Differential Equations</em> tells us that a solution to an initial value problem always exists as a few (relatively weak) conditions are met.</p>
<p>Consider the following ODE: 
$$
y' = F(t, y), y(t0) = y0
$$
</p>
<p>When F is a continuous function on some domain <br /><span class="math display"><em>Ω</em></span><br /> in the Cartesian plane, there exists a certain range in that plane around the starting point where a function exists that solves that ODE. Moreover, if \(F_y\) is continuous, then that solution will be unique.</p>
<p>This theorem is important for a number of reasons:</p>
<ol type="1">
<li>It guarantees that for “nice” functions, a solution to the ODE will always exist, even if we might not be able to solve for it.</li>
<li>It guarantees that that solution will be unique if \(F_y\) is continuous, which is often more important than knowing what that solution is.</li>
<li>It tells us that we can determine the maximal domain of the function solution without actually solving for the function– because that domain is precisely the minimum domain among \(F()\) and \(y()\).</li>
<li>From this theorem we can derive information about how the solutions to an ODE <em>vary</em> as the initial conditions vary. They do so smoothly, like a piece of hair being drawn out on a curler… and futhermore, solutions to the same ODE at different initial conditions (also known as integral lines) can be transformed (rectified) by an invertible function to become parallel lines.</li>
</ol>
<p>However, keep in mind that this theorem is only actually useable under certain, specific circumstances– as always, watch out for discontinuities.</p>
<p>So far, we’ve only covered solutions to ODEs based on <em>analytical</em> methods, i.e. that are created by manipulating symbols. For many equations, this is impossible, and so we resor to numerical methods,. The upside of these is that they are valid for any differential equation. The downside is that they are mere approximations.</p>
<h1 id="euler-method">Euler Method</h1>
<p>Consider the following initial value problem:</p>
<p>$$y’ = F(t, y)$$</p>
<p>$$y(t0) = y0$$</p>
<p>You can use Euler’s method to create a polygonal path that approximates the function on which it is defined.</p>
<p>You can show that there exists a sequence such that the limit of this polygonal path, as the number of polygons grows from more to more, is the original function that we were trying to approximate.</p>
<p>Remember that Euler’s method means:</p>
$$
y_(t+1) = y(t) + F(t, y(t)) \cdot h
$$
<p>Where h is the step size. The approximation becomes better as h decreases. Interestingly, the difference between the value of the differential equation at any point according to Euler’s method and its actual value at that point is linearly bounded by h:</p>
$$
|\phi_h(T) - \phi(T)| < Kh
$$
<p>K is a constant that varies based on the function F and the length of the total interval between the initial value and the point of extrapolation.</p>
<h1 id="picard-method">Picard Method</h1>
<p>The Euler Method approximates a point along a curve by tip-toeing along the curve’s edge towards the target number. The Picard Method chooses to approximate functions instead: given an initial guess <br /><span class="math display">$$\y(t)$$</span><br /> for a solution in terms of y, the Picard Method can produce a better guess <em>ad infinitum</em> until we reach the desired accuracy.</p>
<p>The general formula:</p>
$$
y(x) = y_0 + \Integral_(x_0)^x f(t, y(t)) dt
$$
<p>Your initial guess for y(x) can be as bad as you want it to be, it can even be a constant. Plug it into the integral as <span class="math inline"><em>y</em>(<em>x</em>)</span> and the Picard method will produce something better. If your ODE is of a certain form, for example a linear ODE, successive iterations of Picard’s Method will actually produce a polynomial which could end up being the Taylor series expansion of the analytic solution.</p>
<p>The disadvantages of this method are that it converges slowly and is difficult to do by hand (unlike the Euler method). However, it does have the advantage of replacing the continuity condition with the weaker <strong>Lipschitz condition</strong>, which is true iff there exists a constant K such that</p>
$$
|f(x_1) - f(x_2)| < K(x_1 - x_2)
$$
<p>In other words, a function meets the Lipschitz condition at a point if all its other points lie within the two sectors of the Cartesian plane defined by lines of slope +K and -K extending from that point.</p>


        </div>
        <div class="bottomScroll"> </div>
        <div class="earthWrapper">
            <img src="../art/earthInSpaceTwiceCropped.png">
        </div>
        

    </body>
</html>
